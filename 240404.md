svm에서 c와 gamma
## decision tree
> 클래스로 나눠!
> 스케일 조정하지 않은 특성 사용가능, 전처리 하지 않아도 된다.

<Br>
 https://blog.naver.com/PostView.naver?blogId=gdpresent&logNo=221723178689

![decisiontree](https://velog.velcdn.com/images%2Fkatinon%2Fpost%2F5ad06de1-9e07-431e-8ccb-35a0de51113c%2Fimage.png)

```python
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
```
![depth](https://velog.velcdn.com/images%2Fkatinon%2Fpost%2Fe7660721-dbec-4dc3-91a1-d0cedd14affa%2Fimage.png)

분류 : 루트->리프 까지의 다수의 클래스가 예측모델
회귀 : 마지막 리프노드의 타깃값이 샘플의 예측값

불순도의 성과가 동일할때:
random하게 a,b를 선택
gini 불순도: 
https://www.youtube.com/watch?v=tOzxDGp8rsg
32.34초
리프노드가 순수노드까지 될때까지
지니불순도가 0이 되도록 쪼갠다!


결정트리 성장시키면 과대적합. 줄이면 성능이 덜나온다.

트리로 앙상블 모델 만드는것이 목적이다.

## 교차검증
>딥러닝은 데이터가 충분하기때문에 검증을 하지 않아도됨
5폴드     
검증세트 갯수가 너무 작으면 불안정 하지만
검증세트를 여러번 반복하면 안정되어짐.

## 그리드서치  
max depth와 min_inpurity_decrease 최적점 찾아야함.    

거의 모두 그리드서치 사용.
매개변수를 딕셔너리로 사용.


하이퍼파라메터 : 모델을 생성할때 사용자가 직접 설정하는 변수    
- 랜덤포레스트 모델의 경우 트리의 개수를 몇개까지 할지, 깊이는 몇까지 할지 
- 딥러닝의 경우 layer개수, 에포크 수...    

파라미터 : 학습과정에서 생성되는 변수들.
>그리드 서치를 하는 이유는 가장 우수한 성능을 보이는 모델의 하이퍼 파라미터를 찾기 위해서
단점: 시간이 많이 걸린다.

 




